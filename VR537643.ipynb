{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bed95dd",
   "metadata": {},
   "source": [
    "# Homework 1 & 2\n",
    "\n",
    "Elia Mirafiori VR537643\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "### Acquisition of Calibration Images\n",
    "\n",
    "To acquire the images, I've used my phone camera and as a pattern I've opted for a checkerboard 8x8 squares with 7x7 internal corners (the size of a square cell is 24mm per side). There have been collected 14 images to calibrate the camera with different poses and varying distances. All the images have been saved in JPG format.\n",
    "\n",
    "### Choice and Acquisition of the Object\n",
    "\n",
    "The object choosen for the 3D reconstruction is a Moai statue (it's a tissue holder). The reason why I chose it, it's because it rch in features and based on different light conditions, the detection and the matching of common points can vary widely. So, it's a good objection to test different setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ab8f0",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "In order to run the code, I needed a few 3-rd party libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef640b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21297697",
   "metadata": {},
   "source": [
    "## Camera Calibration\n",
    "\n",
    "As previously mentioned, I used a checkerboard to calibrate the camera. The criteria I employed were a maximum ceil of iterations (_30_) and a minimum required accuracy (_0.001_).\n",
    "\n",
    "The procedure it's straight forward. I extrapolate world points (_3D_) and map it in image points (_2D_). To find the world points, I employed `cv.findChessboardCorners` that returns if the checkerboard corners have been found, if yes their coordinates get returned as well. Then, to get the image points I employed `cv.cornerSubPix` with the criteria specified before, a window size of _(11, 11)_ and a dead zone of _(-1, -1)_.\n",
    "\n",
    "In the end, I employed the `cv.calibrateCamera` to retrieve the Camera matrix $\\mathbf{K}$ and the Distortion Coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d606e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(\n",
    "    calibration_assets_path: str = \"assets/calibration/phone/vertical\",\n",
    "    columns: int = 8,\n",
    "    rows: int = 8,\n",
    "    square_size: float = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibration function\n",
    "    \"\"\"\n",
    "\n",
    "    # Termination criteria for corner refinement (sub-pixel accuracy)\n",
    "    # Stops when either:\n",
    "    #  - max iterations are reached\n",
    "    #  - or the desired accuracy is achieved\n",
    "    criteria = (\n",
    "        cv.TERM_CRITERIA_MAX_ITER + cv.TERM_CRITERIA_EPS,\n",
    "        30,  # max number of iterations\n",
    "        0.001,  # minimum required accuracy (epsilon)\n",
    "    )\n",
    "\n",
    "    # Chessboard configuration\n",
    "    inner_corners = (columns - 1, rows - 1)  # number of INNER corners (columns, rows)\n",
    "\n",
    "    # Prepare 3D object points in real-world coordinates\n",
    "    # The chessboard lies on the Z = 0 plane\n",
    "    objp = np.zeros((inner_corners[0] * inner_corners[1], 3), np.float32)\n",
    "\n",
    "    # Generate grid and scale it by the square size (meter)\n",
    "    # ':'\tAll rows, ':2'\tFirst two columns only (index 0 and 1)\n",
    "    objp[:, :2] = (\n",
    "        np.mgrid[0 : inner_corners[0], 0 : inner_corners[1]].T.reshape(-1, 2)\n",
    "        * square_size\n",
    "    )\n",
    "\n",
    "    # Containers for calibration points\n",
    "    objpoints = []  # 3D points in real-world space (meter)\n",
    "    imgpoints = []  # 2D points in image plane (pixels)\n",
    "\n",
    "    # Load all calibration images from disk\n",
    "    # Each image should show the same chessboard pattern\n",
    "    images = glob.glob(f\"{calibration_assets_path}/*.jpg\")\n",
    "\n",
    "    # Loop over each calibration image\n",
    "    for img_path in images:\n",
    "        print(f\"Path:\\n\\t{img_path}\")\n",
    "\n",
    "        # Read image from disk (OpenCV loads images in BGR format)\n",
    "        img_bgr = cv.imread(img_path)\n",
    "\n",
    "        # Convert image to grayscale\n",
    "        # Chessboard detection works on single-channel images\n",
    "        img_gray = cv.cvtColor(img_bgr, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect chessboard inner corners\n",
    "        corners_found, corners = cv.findChessboardCorners(img_gray, inner_corners, None)\n",
    "\n",
    "        print(f\"Corners found:\\n\\t{corners_found}\")\n",
    "\n",
    "        # If the chessboard was successfully detected\n",
    "        if corners_found:\n",
    "\n",
    "            # Store the known 3D object points (real-world coordinates)\n",
    "            # Same for every image, since the chessboard geometry is fixed\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            # Refine corner positions to sub-pixel accuracy\n",
    "            #\n",
    "            # This improves calibration precision significantly\n",
    "            #\n",
    "            # (11, 11)  -> search window size\n",
    "            # (-1, -1)  -> use default dead zone\n",
    "            # criteria  -> termination criteria defined earlier\n",
    "            corners_refined = cv.cornerSubPix(\n",
    "                img_gray, corners, (11, 11), (-1, -1), criteria\n",
    "            )\n",
    "\n",
    "            # Store the refined 2D image points (pixel coordinates)\n",
    "            imgpoints.append(corners_refined)\n",
    "\n",
    "            # Visual feedback: draw detected corners on the image\n",
    "            cv.drawChessboardCorners(\n",
    "                img_bgr, inner_corners, corners_refined, corners_found\n",
    "            )\n",
    "\n",
    "            # Display the image briefly\n",
    "            cv.imshow(\n",
    "                \"Calibration Image\",\n",
    "                cv.resize(\n",
    "                    img_bgr,\n",
    "                    (img_bgr.shape[1] // 4, img_bgr.shape[0] // 4),\n",
    "                ),\n",
    "            )\n",
    "            cv.waitKey(500)  # display for 500 ms\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    image_shape = cv.imread(images[0]).shape[:2][::-1]  # width, height\n",
    "\n",
    "    rms_error, K, dist_coeffs, rot_vecs, trans_vecs = cv.calibrateCamera(\n",
    "        objpoints, imgpoints, image_shape, None, None\n",
    "    )\n",
    "\n",
    "    # Compute the mean re-projection error (in pixels) over the calibration images\n",
    "    errors = []\n",
    "\n",
    "    for objp, imgp, rvec, tvec in zip(objpoints, imgpoints, rot_vecs, trans_vecs):\n",
    "        projected, _ = cv.projectPoints(objp, rvec, tvec, K, dist_coeffs)\n",
    "        projected = projected.reshape(-1, 2)\n",
    "        imgp = imgp.reshape(-1, 2)\n",
    "\n",
    "        # Euclidean pixel error per point\n",
    "        err = np.linalg.norm(imgp - projected, axis=1)\n",
    "        errors.append(err)\n",
    "\n",
    "    errors = np.concatenate(errors)\n",
    "\n",
    "    mean_error = np.mean(errors)\n",
    "    std_error = np.std(errors)\n",
    "\n",
    "    print(f\"\\nCamera Matrix K:\\n{K}\")\n",
    "    print(f\"Re-projection Error (in pixels):\\n\\t{rms_error}\")\n",
    "    # The error is good when it's under 0.08\n",
    "    print(f\"Mean reprojection error: {mean_error:.3f} px\")\n",
    "    print(f\"Std dev reprojection error: {std_error:.3f} px\")\n",
    "\n",
    "    # Iterate over all images to show their individual poses\n",
    "    for i, (r_vec, t_vec) in enumerate(zip(rot_vecs, trans_vecs)):\n",
    "        R, _ = cv.Rodrigues(r_vec)\n",
    "        print(f\"\\nImage {i} Pose\")\n",
    "        print(f\"Rotation Matrix R:\\n{R}\")\n",
    "        print(f\"Translation t:\\n{t_vec}\")\n",
    "\n",
    "    # Save calibration parameters\n",
    "    param_path = os.path.join(calibration_assets_path, \"calibration.npz\")\n",
    "\n",
    "    # Save several arrays into a single file in uncompressed .npz format\n",
    "    np.savez(\n",
    "        param_path,\n",
    "        rms_error=rms_error,\n",
    "        K=K,\n",
    "        dist_coeffs=dist_coeffs,\n",
    "        rot_vecs=rot_vecs,\n",
    "        trans_vecs=trans_vecs,\n",
    "    )\n",
    "\n",
    "    return K, dist_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad0f41",
   "metadata": {},
   "source": [
    "## Feature Detection & Matching\n",
    "\n",
    "### Feature Detection\n",
    "\n",
    "In order to reconstruct the object, I needed to pass 2 images of the object that differ by a translation and a rotation from each other. Given the input images, I've setup some detectors (ORB, BRISK, AKAZE, SIFT) to test their performances over the same images. Then, I needed the Camera matrix and the Distotion Coefficients to undistort the images and have a cleaner result.\n",
    "\n",
    "I employed `cv.DETECTOR_create` to create a detector object and detector.detectAndCompute to start the detection and computation over the input images, this procedure finds **Keypoints** (points of interest like corners/edges) and calculates **Descriptors** (binary vectors that describe the area around the keypoint). These will be used to in all the further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f067f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_detection(\n",
    "    img1_path: str,\n",
    "    img2_path: str,\n",
    "    detector=None,\n",
    "    K=None,\n",
    "    dist=None,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects features in two images using ORB and routes them to a matcher.\n",
    "    \"\"\"\n",
    "\n",
    "    # Health check\n",
    "    assert os.path.exists(img1_path), f\"Left image not found: {img1_path}\"\n",
    "    assert os.path.exists(img2_path), f\"Right image not found: {img2_path}\"\n",
    "\n",
    "    # Loads in GRAYSCALE because feature detection relies on intensity changes (gradients).\n",
    "    # Color information is usually unnecessary for this and adds computational cost (3 channels vs 1).\n",
    "    img1 = cv.imread(img1_path, cv.IMREAD_GRAYSCALE)  # Query image (left)\n",
    "    img2 = cv.imread(img2_path, cv.IMREAD_GRAYSCALE)  # Train image (right)\n",
    "\n",
    "    if K is not None and dist is not None:\n",
    "        if debug:\n",
    "            print(\"Undistorting images before detection...\")\n",
    "\n",
    "        # Refine camera matrix to avoid losing pixels at the edges\n",
    "        # h, w = img1.shape[:2]\n",
    "        # new_camera_matrix, roi = cv.getOptimalNewCameraMatrix(K, dist, (w,h), 1, (w,h))\n",
    "\n",
    "        # Undistort\n",
    "        img1 = cv.undistort(img1, K, dist, None, K)\n",
    "        img2 = cv.undistort(img2, K, dist, None, K)\n",
    "\n",
    "    # Initialize Detector\n",
    "    # 'nfeatures=5000' is the maximum number of keypoints to retain.\n",
    "    # The default is often 500, but 5000 is better for high-res images or detailed scenes\n",
    "    # to ensure enough matches are found later.\n",
    "    if detector is None:\n",
    "        detector = cv.ORB_create(nfeatures=10000)\n",
    "\n",
    "    # Detection & Description\n",
    "    # detectAndCompute performs two steps:\n",
    "    #   1. Detect: Finds 'Keypoints' (points of interest like corners/edges)\n",
    "    #   2. Compute: Calculates 'Descriptors' (binary vectors that describe the area around the keypoint)\n",
    "    # The 'mask=None' argument means we look for features in the entire image.\n",
    "    kp1, des1 = detector.detectAndCompute(img1, None)\n",
    "    kp2, des2 = detector.detectAndCompute(img2, None)\n",
    "\n",
    "    # akaze = cv.AKAZE_create(threshold=0.0005)\n",
    "    #\n",
    "    # kp1, des1 = akaze.detectAndCompute(img1, None)\n",
    "    # kp2, des2 = akaze.detectAndCompute(img2, None)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Detected {len(kp1)} keypoints in left image.\")\n",
    "        print(f\"Detected {len(kp2)} keypoints in right image.\")\n",
    "\n",
    "    # Safety Check\n",
    "    # If an image has no texture (e.g., a blank wall), descriptors might be None.\n",
    "    # Proceeding without this check would cause the matchers to crash.\n",
    "    if des1 is None or des2 is None:\n",
    "        raise RuntimeError(\n",
    "            \"Descriptors are None. Try different images or a different feature extractor.\"\n",
    "        )\n",
    "\n",
    "    return img1, kp1, des1, img2, kp2, des2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed90bf8",
   "metadata": {},
   "source": [
    "### Feature Matcher\n",
    "\n",
    "After the feature detection, I needed to match the keypoints of the input images. To do so, I've used a set of matchers (Brute-Force, FLANN). In the end, I compared the results of the pair _DETECTOR-MATCHER_. I employed `cv.NAMEMatcher` to create the matcher object.\n",
    "\n",
    "For the Brute-Force matcher, I've sorted the matches and filtered out the ones with an higher distance.\n",
    "\n",
    "Meanwhile, for the FLANN matcher, I've applied the KNN match and the Lowe's Ratio test.\n",
    "\n",
    "In order to keep a reasonable amount of points, I filtered out and kept only at max 5000 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matcher(\n",
    "    img1,\n",
    "    kp1,\n",
    "    des1,\n",
    "    img2,\n",
    "    kp2,\n",
    "    des2,\n",
    "    matcher=None,\n",
    "    max_matches: int = 5000,\n",
    "    ratio_test: float = 0.70,\n",
    "):\n",
    "    \"\"\"\n",
    "    Matches descriptors using the provided matcher.\n",
    "    Automatically applies ratio test for FLANN matchers and optionally for BFMatcher.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Matcher\n",
    "    # cv.NORM_HAMMING: Essential for binary descriptors (ORB, BRIEF).\n",
    "    # It calculates distance by counting the number of differing bits (XOR operation).\n",
    "    # crossCheck=True: This enforces a mutual match.\n",
    "    # Feature A in img1 must match B in img2, AND Feature B in img2 must match A in img1.\n",
    "    # This filters out many false positives automatically.\n",
    "    if matcher is None:\n",
    "        matcher = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # Check if matcher is FLANN or BF\n",
    "    matcher_type = type(matcher).__name__\n",
    "\n",
    "    good_matches = []\n",
    "\n",
    "    # Perform Matching\n",
    "    # A DMatch object has four main attributes:\n",
    "    # Match(\n",
    "    #    queryIdx,  # index of the descriptor in the first set (des1)\n",
    "    #    trainIdx,  # index of the descriptor in the second set (des2)\n",
    "    #    imgIdx,    # index of the training image (used with multiple images)\n",
    "    #    distance   # distance between the two descriptors\n",
    "    # )\n",
    "    if matcher_type == \"FlannBasedMatcher\":\n",
    "        # FLANN: use knnMatch + ratio test\n",
    "        matches = matcher.knnMatch(des1, des2, k=2)\n",
    "\n",
    "        # Need to draw only good matches, so create a mask\n",
    "        matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "        # Ratio test as per Lowe's paper\n",
    "        for i, (m, n) in enumerate(matches):\n",
    "            if m.distance < ratio_test * n.distance:\n",
    "                good_matches.append(m)\n",
    "    else:\n",
    "        # BFMatcher\n",
    "        if getattr(matcher, \"crossCheck\", True):\n",
    "            # Cross-checked BF: use match()\n",
    "            matches = matcher.match(des1, des2)\n",
    "            # Lower distance = more similar descriptors = better match.\n",
    "            good_matches = sorted(matches, key=lambda x: x.distance)\n",
    "        else:\n",
    "            # Non-cross-checked BF: use knnMatch + ratio test\n",
    "            matches = matcher.knnMatch(des1, des2, k=2)\n",
    "\n",
    "            # Ratio test as per Lowe's paper\n",
    "            for m, n in matches:\n",
    "                if m.distance < ratio_test * n.distance:\n",
    "                    good_matches.append(m)\n",
    "            # Lower distance = more similar descriptors = better match.\n",
    "            good_matches = sorted(good_matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Selection\n",
    "    # We slice the list to keep only the top N matches.\n",
    "    # This removes weak matches that might just be noise or repetitive textures.\n",
    "    good_matches = good_matches[: min(max_matches, len(good_matches))]\n",
    "\n",
    "    print(\n",
    "        f\"Using {len(good_matches)} best matches for fundamental matrix estimation.\"\n",
    "    )\n",
    "\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82179fa4",
   "metadata": {},
   "source": [
    "## Visualize Matcher\n",
    "\n",
    "To visualize the Feature Detection and Matching, I've setup a simple function that requries the input images with their keypoints and matches. Then, I employed `cv.drawMatches` to draw the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95aa70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matcher(img1, kp1, img2, kp2, matches):\n",
    "\n",
    "    # Visualization\n",
    "    matches_to_show = min(100, len(matches))\n",
    "\n",
    "    # drawMatches creates a new image containing both img1 and img2 side-by-side\n",
    "    # and draws lines connecting the matched keypoints.\n",
    "    img_matches = cv.drawMatches(\n",
    "        img1,\n",
    "        kp1,\n",
    "        img2,\n",
    "        kp2,\n",
    "        matches[:matches_to_show],\n",
    "        None,\n",
    "        flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,  # Only draw matched points, not all detected ones\n",
    "    )\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title(\"Feature matches (subset)\")\n",
    "    plt.imshow(img_matches)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cde0f3",
   "metadata": {},
   "source": [
    "## Keypoint coordinates from Matches\n",
    "\n",
    "In the next steps, I needed the coordinates of the matches, so with this function I retrieved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a77901fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoint_coords_from_matches(kp1, kp2, matches):\n",
    "    \"\"\"\n",
    "    Helper to extract coordinates from matches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Point Extraction\n",
    "    # Convert matched keypoints into simple numpy arrays of (x, y) coordinates.\n",
    "    # pts1: Points in the Left Image (from queryIdx)\n",
    "    # pts2: Points in the Right Image (from trainIdx)\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    return pts1, pts2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1d87c",
   "metadata": {},
   "source": [
    "## Epipolar Geometry\n",
    "\n",
    "### Fundamental Matrix\n",
    "\n",
    "In order to derive the Essential matrix $\\mathbf{E}$, I had two possible approaches. The **Indirect Approach** requires the Fundamental matrix $\\mathbf{F}$, then with the Camera matrix found previously I could compute it by doing $\\mathbf{E} = \\mathbf{K}^\\top \\mathbf{F} \\mathbf{K}$. The results obtained with this approach were pretty similar to the ones got with the **Direct Approach**. The latter is the one I've implemented below.\n",
    "\n",
    "The Fundamental matrix has been computed with **RANSAC** (threshold of _1_ and a confidence of _0.999_). I employed the `cv.findFundamentalMat` function that gave me also a mask to filter out the outliers.\n",
    "\n",
    "The remaining polished points were used to compute the Essential matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82048c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_fundamental_matrix(\n",
    "    pts1,\n",
    "    pts2,\n",
    "    ransac_thresh=1.0,\n",
    "    confidence=0.999,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimates the Fundamental Matrix F using RANSAC and visualizes inliers.\n",
    "    \"\"\"\n",
    "    # Fundamental Matrix Estimation\n",
    "    # This calculates the relationship between pixel coordinates in two views.\n",
    "    # cv.FM_RANSAC: Uses Random Sample Consensus to ignore outliers (bad matches).\n",
    "    # ransacReprojThreshold: The maximum distance (in pixels) a point can be from the epipolar line to be considered an inlier.\n",
    "    F, mask = cv.findFundamentalMat(\n",
    "        pts1,\n",
    "        pts2,\n",
    "        cv.FM_RANSAC,\n",
    "        ransacReprojThreshold=ransac_thresh,\n",
    "        confidence=confidence,\n",
    "    )\n",
    "\n",
    "    if F is None:\n",
    "        raise RuntimeError(\"Fundamental matrix estimation failed\")\n",
    "\n",
    "    # Filtering Inliers\n",
    "    # \"mask\" is a list of 0s (outliers) and 1s (inliers).\n",
    "    # We flatten it to a 1D array of booleans.\n",
    "    mask = mask.ravel().astype(bool)\n",
    "\n",
    "    # Select ONLY the points that agree with the Fundamental Matrix geometry.\n",
    "    pts1_in = pts1[mask]\n",
    "    pts2_in = pts2[mask]\n",
    "\n",
    "\n",
    "    return F, pts1_in, pts2_in, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170580e",
   "metadata": {},
   "source": [
    "### Essential Matrix\n",
    "\n",
    "As mentioned before, I've computed the Essential matrix by employing the `cv.findEssentialMat` function. It required as an input the matched points and the Camera matrix. I've computed it with **RANSAC** (threshold of _1_ and a confidence level of _0.999_). Also this function returned a mask to filter out the outliers.\n",
    "\n",
    "The remaining polished points were used in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75213388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_essential_matrix(\n",
    "    pts1,\n",
    "    pts2,\n",
    "    K,\n",
    "    prob=0.999,\n",
    "    threshold=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimates the Essential Matrix E using RANSAC and the Camera Matrix K.\n",
    "    \"\"\"\n",
    "\n",
    "    # Essential Matrix Estimation\n",
    "    # method=cv.RANSAC: Standard RANSAC\n",
    "    # prob=0.999: Confidence level (higher is better for E)\n",
    "    # threshold=1.0: Max distance from epipolar line (in pixels)\n",
    "    E, mask = cv.findEssentialMat(\n",
    "        pts1,\n",
    "        pts2,\n",
    "        K,\n",
    "        method=cv.RANSAC,\n",
    "        prob=prob,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if E is None:\n",
    "        raise RuntimeError(\"Essential matrix estimation failed\")\n",
    "\n",
    "    # Flatten the mask\n",
    "    # \"mask\" is a list of 0s (outliers) and 1s (inliers).\n",
    "    # We flatten it to a 1D array of booleans.\n",
    "    mask = mask.ravel().astype(bool)\n",
    "\n",
    "    # Select ONLY the points that agree with the Fundamental Matrix geometry.\n",
    "    pts1_in = pts1[mask]\n",
    "    pts2_in = pts2[mask]\n",
    "\n",
    "    return E, pts1_in, pts2_in, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c9b01",
   "metadata": {},
   "source": [
    "### Epipolar Geometry Visualization\n",
    "\n",
    "For every point in one image, the Fundamental Mmtrix determines a line in the other image on which the corresponding point **must** lie.\n",
    "\n",
    "To accomplish this task, I employed the `cv.computeCorrespondEpilines` function on both input images, then I drew the lines over them to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4127d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_epipolar_lines(img1, pts1, img2, pts2, F):\n",
    "    \"\"\"\n",
    "    Visualizes epipolar lines.\n",
    "    For every point in one image, the Fundamental Matrix determines a line in the other image\n",
    "    on which the corresponding point MUST lie.\n",
    "    \"\"\"\n",
    "\n",
    "    # Right-to-Left Epilines\n",
    "    # We take points from the RIGHT image (pts2) and calculate where their\n",
    "    # corresponding epipolar lines should be in the LEFT image.\n",
    "    # The '2' argument tells OpenCV the points are from the second image.\n",
    "    # F maps p2 -> line1 (l = F.T * p2)\n",
    "    lines1 = cv.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)\n",
    "    lines1 = lines1.reshape(\n",
    "        -1, 3\n",
    "    )  # Reshape to N x 3 (a, b, c for line ax + by + c = 0)\n",
    "\n",
    "    # Draw these lines on img1 (Left Image)\n",
    "    # img5 will show lines on Left Image, img6 shows the points on Right Image\n",
    "    img5, img6 = drawlines(img1, img2, lines1, pts1, pts2)\n",
    "\n",
    "    # Left-to-Right Epilines\n",
    "    # We take points from the LEFT image (pts1) and find lines in the RIGHT image.\n",
    "    # The '1' argument tells OpenCV the points are from the first image.\n",
    "    # F maps p1 -> line2 (l = F * p1)\n",
    "    lines2 = cv.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)\n",
    "    lines2 = lines2.reshape(-1, 3)\n",
    "\n",
    "    # Draw these lines on img2 (Right Image)\n",
    "    # img3 will show lines on Right Image, img4 shows the points on Left Image\n",
    "    img3, img4 = drawlines(img2, img1, lines2, pts2, pts1)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(121), plt.imshow(img5)\n",
    "    plt.title(\"Epilines on Left Image\"), plt.axis(\"off\")\n",
    "    plt.subplot(122), plt.imshow(img3)\n",
    "    plt.title(\"Epilines on Right Image\"), plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def drawlines(img1, img2, lines, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Helper function to draw lines on one image and matching points on the other.\n",
    "\n",
    "    Args:\n",
    "        img1: Image on which we draw the epilines (lines).\n",
    "        img2: Image on which we draw the points (pts2) that generated those lines.\n",
    "        lines: The epipolar lines coefficients (a, b, c) where ax + by + c = 0\n",
    "        pts1: Points in img1 (that should fall ON the lines).\n",
    "        pts2: Points in img2 (that generated the lines via F).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get image dimensions (rows, cols) to calculate start/end points of lines\n",
    "    r, c = img1.shape\n",
    "\n",
    "    # Convert to BGR so we can draw colored lines/circles\n",
    "    img1 = cv.cvtColor(img1, cv.COLOR_GRAY2BGR)\n",
    "    img2 = cv.cvtColor(img2, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Iterate through lines and points simultaneously\n",
    "    # 'lines' corresponds to pts2 (source) and matches pts1 (destination)\n",
    "    for r, pt1, pt2 in zip(lines, pts1, pts2):\n",
    "\n",
    "        # Random color for each pair to distinguish them\n",
    "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "\n",
    "        # --- Line Calculation ---\n",
    "        # Epipolar line equation: ax + by + c = 0\n",
    "        # We need two points (x0, y0) and (x1, y1) to draw the line segment.\n",
    "        # r[0]=a, r[1]=b, r[2]=c\n",
    "\n",
    "        # Point 1 (Left edge, x=0):\n",
    "        # a(0) + by + c = 0  =>  by = -c  =>  y = -c/b\n",
    "        x0, y0 = map(int, [0, -r[2] / r[1]])\n",
    "\n",
    "        # Point 2 (Right edge, x=c (width)):\n",
    "        # a(c) + by + c = 0  =>  by = -(c + ac)  =>  y = -(c + ac)/b\n",
    "        # Note: variable 'c' here is image width (cols), r[2] is line coefficient 'c'\n",
    "        x1, y1 = map(int, [c, -(r[2] + r[0] * c) / r[1]])\n",
    "\n",
    "        # Draw the epipolar line on the target image\n",
    "        img1 = cv.line(img1, (x0, y0), (x1, y1), color, 1)\n",
    "\n",
    "        # Draw the corresponding point on the target image (should be ON the line)\n",
    "        # Note: pt1 needs to be integer tuple for cv.circle\n",
    "        img1 = cv.circle(img1, tuple(np.int32(pt1)), 5, color, -1)\n",
    "\n",
    "        # Draw the source point on the source image (just for reference)\n",
    "        img2 = cv.circle(img2, tuple(np.int32(pt2)), 5, color, -1)\n",
    "\n",
    "    return img1, img2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438636a",
   "metadata": {},
   "source": [
    "## Recover Pose\n",
    "\n",
    "In order to compute the **Projection** matrices $\\mathbf{P1}$ and $\\mathbf{P2}$, I first needed to recover the **Rotation** matrix $\\mathbf{R}$ and the Translation vector $\\mathbf{t}$. I employed `cv.recoverPose` from OpenCV, it uses the **inliers** points (_pts1_, _pts2_) to check which of the 4 solutionsis valid.\n",
    "\n",
    "```python\n",
    "num_points, R, t, mask = cv.recoverPose(E, pts1_in, pts2_in, K)\n",
    "```\n",
    "\n",
    "`cv.recoverPose` returns a mask that I used to filter points again. This approach helped a lot to get a more polished reconstruction.\n",
    "\n",
    "```python\n",
    "mask_pose = mask.ravel().astype(bool)\n",
    "pts1_valid = pts1_in[mask_pose]\n",
    "pts2_valid = pts2_in[mask_pose]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e39a0d",
   "metadata": {},
   "source": [
    "## Projection Matrices\n",
    "\n",
    "As previously stated, to compute the Projection matrices I've retrieved the Rotation matrix and the Translation vector. Then, I've computed them by using the following formula $$\\mathbf{P1} = \\mathbf{K} \\left[ \\mathbf{I} \\mid \\mathbf{0} \\right], \\quad\n",
    "\\mathbf{P2} = \\mathbf{K} \\left[ \\mathbf{R} \\mid \\mathbf{t} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7155f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_projection_matrices(K, R, t):\n",
    "    \"\"\"\n",
    "    Computes P1 and P2 from the Essential Matrix using recoverPose.\n",
    "    \"\"\"\n",
    "    # Construct P1 (Origin)\n",
    "    # P1 = K @ [I | 0]\n",
    "    # Identity matrix (3x3) concatenated with a zero column (3x1)\n",
    "    I = np.eye(3)\n",
    "    zeros = np.zeros((3, 1))\n",
    "    P1 = np.dot(K, np.hstack((I, zeros)))\n",
    "\n",
    "    # Construct P2 (New View)\n",
    "    # P2 = K @ [R | t]\n",
    "    P2 = np.dot(K, np.hstack((R, t)))\n",
    "\n",
    "    # Return P1, P2 AND the points that actually define this pose\n",
    "    return P1, P2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9cca1",
   "metadata": {},
   "source": [
    "## Triangulation\n",
    "\n",
    "To build the 3D reconstruction, I needed to pass from the 2D image points to the 3D real-world points.\n",
    "\n",
    "Given the Projection matrices and the inliers, I employed `cv.triangulatePoints` to get a _4xN_ matrix of homogeneous coordinates _(x, y, z, w)_. Then, I had to convert **Homogeneous** (_4D_) coordinates to **Euclidean** (_3D_) coordinates by dividing _x_, _y_, and _z_ by _w_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "271c7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_3d_points(P1, P2, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulates 2D points from two views into 3D space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transpose points\n",
    "    # cv.triangulatePoints expects data in shape (2, N),\n",
    "    # but our points are currently (N, 2).\n",
    "    pts1_t = pts1.T\n",
    "    pts2_t = pts2.T\n",
    "\n",
    "    # Triangulate\n",
    "    # This returns a 4xN matrix of homogeneous coordinates (x, y, z, w)\n",
    "    points_4d = cv.triangulatePoints(P1, P2, pts1_t, pts2_t)\n",
    "\n",
    "    # Convert Homogeneous (4D) to Euclidean (3D)\n",
    "    # We must divide x, y, and z by w.\n",
    "    points_3d = points_4d[:3, :] / points_4d[3, :]\n",
    "\n",
    "    # Transpose back to (N, 4) and take the first 3 columns (x, y, z)\n",
    "    points_3d = points_3d[:3].T\n",
    "\n",
    "    return points_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb129c",
   "metadata": {},
   "source": [
    "## Reprojection error\n",
    "\n",
    "This step is just a quantitative result and I used it to compute the average re-projection error (RMSE) in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2c62f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reprojection_error(P1, P2, points_3d, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Computes the average re-projection error (RMSE) in pixels.\n",
    "    \"\"\"\n",
    "    # Convert 3D points to homogeneous coordinates (4xN)\n",
    "    # Shape becomes (4, N) -> [[x, y, z, 1], ...]\n",
    "    points_3d_hom = np.hstack((points_3d, np.ones((points_3d.shape[0], 1)))).T\n",
    "\n",
    "    # Project into Image 1\n",
    "    # P1 is 3x4, points_3d_hom is 4xN -> projected_1 is 3xN\n",
    "    projected_1_hom = np.dot(P1, points_3d_hom)\n",
    "\n",
    "    # Normalize by the last row (Z/W) to get pixel coordinates (u, v)\n",
    "    projected_1 = projected_1_hom[:2] / projected_1_hom[2]\n",
    "\n",
    "    # Calculate Euclidean distance between observed and projected points\n",
    "    # pts1 is (N, 2), projected_1.T is (N, 2)\n",
    "    error_1 = np.linalg.norm(pts1 - projected_1.T, axis=1)\n",
    "\n",
    "    # Project into Image 2\n",
    "    projected_2_hom = np.dot(P2, points_3d_hom)\n",
    "    projected_2 = projected_2_hom[:2] / projected_2_hom[2]\n",
    "    error_2 = np.linalg.norm(pts2 - projected_2.T, axis=1)\n",
    "\n",
    "    # Compute Mean Error\n",
    "    mean_error_1 = np.mean(error_1)\n",
    "    mean_error_2 = np.mean(error_2)\n",
    "    total_mean_error = (mean_error_1 + mean_error_2) / 2.0\n",
    "\n",
    "    return mean_error_1, mean_error_2, total_mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea3f22",
   "metadata": {},
   "source": [
    "## 3D Reconstruction\n",
    "\n",
    "I used this step to visualize the 3D point cloud using Matplotlib with depth-based coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85aa2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_point_cloud(points_3d):\n",
    "    \"\"\"\n",
    "    Visualizes the 3D point cloud using Matplotlib with depth-based coloring.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Extract X, Y, Z columns\n",
    "    X = points_3d[:, 0]\n",
    "    Y = points_3d[:, 1]\n",
    "    Z = points_3d[:, 2]\n",
    "\n",
    "    # c=Z          -> Maps the color to the Z value (depth)\n",
    "    # cmap='viridis' -> Sets the gradient scheme (try 'plasma', 'inferno', 'jet')\n",
    "    # depthshade=True -> Optional: adds shading to enhance 3D perception\n",
    "    scatter = ax.scatter(X, Y, Z, c=Z, cmap=\"viridis\", marker=\".\", s=2, depthshade=True)\n",
    "\n",
    "    # Add a color bar to interpret the depth\n",
    "    cbar = plt.colorbar(scatter, ax=ax, pad=0.1, shrink=0.6)\n",
    "    cbar.set_label(\"Depth (Z)\")\n",
    "\n",
    "    ax.set_xlabel(\"X Axis\")\n",
    "    ax.set_ylabel(\"Y Axis\")\n",
    "    ax.set_zlabel(\"Z Axis\")\n",
    "    ax.set_title(\"3D Reconstruction (Colored by Depth)\")\n",
    "\n",
    "    # (Keep your existing scaling logic)\n",
    "    max_range = (\n",
    "        np.array([X.max() - X.min(), Y.max() - Y.min(), Z.max() - Z.min()]).max() / 2.0\n",
    "    )\n",
    "    mid_x = (X.max() + X.min()) * 0.5\n",
    "    mid_y = (Y.max() + Y.min()) * 0.5\n",
    "    mid_z = (Z.max() + Z.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "    # Look from the bottom\n",
    "    ax.view_init(elev=-90, azim=-90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe65d6d",
   "metadata": {},
   "source": [
    "## Main Code\n",
    "\n",
    "Here I've setup the core of my software, you can see the login and how I managed to test some pairs of Detectors and Matchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3eaedd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running calibration...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calibration() got an unexpected keyword argument 'debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m K, dist = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning calibration...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m K, dist = \u001b[43mcalibration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalibration_assets_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massets/calibration/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcamera_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43msquare_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCamera Matrix K:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: calibration() got an unexpected keyword argument 'debug'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if we need to run calibration, or just load it\n",
    "    camera_path = \"phone/vertical\"\n",
    "    calib_path = f\"assets/calibration/{camera_path}/calibration.npz\"\n",
    "    debug = True\n",
    "    detectors_matchers = [\n",
    "        # ORB with lots of features, BFMatcher using Hamming (binary)\n",
    "        (\n",
    "            cv.ORB_create(nfeatures=10000),\n",
    "            cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True),\n",
    "        ),\n",
    "        # ORB with FLANN LSH\n",
    "        (\n",
    "            cv.ORB_create(nfeatures=10000),\n",
    "            cv.FlannBasedMatcher(\n",
    "                dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1), {}\n",
    "            ),  # LSH for binary descriptors\n",
    "        ),\n",
    "        # AKAZE with custom threshold, BFMatcher using Hamming\n",
    "        (\n",
    "            cv.AKAZE_create(threshold=0.0005),\n",
    "            cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True),\n",
    "        ),\n",
    "        # BRISK with BFMatcher using Hamming (binary)\n",
    "        (cv.BRISK_create(), cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)),\n",
    "        # SIFT with BFMatcher using L2 (float descriptors)\n",
    "        (cv.SIFT_create(), cv.BFMatcher(cv.NORM_L2, crossCheck=True)),\n",
    "        # SIFT with FLANN KD-Tree\n",
    "        (\n",
    "            cv.SIFT_create(),\n",
    "            cv.FlannBasedMatcher(\n",
    "                dict(algorithm=1, trees=5), dict(checks=50)\n",
    "            ),  # KD-Tree for float descriptors\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    K, dist = None, None\n",
    "\n",
    "    print(\"Running calibration...\")\n",
    "    K, dist = calibration(\n",
    "        calibration_assets_path=f\"assets/calibration/{camera_path}\",\n",
    "        square_size=0.024,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Camera Matrix K:\\n{K}\\n\")\n",
    "        print(f\"Distortion Coefficients:\\n{dist}\\n\")\n",
    "\n",
    "    for detector, matcher in detectors_matchers:\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        if debug:\n",
    "            detector_type = type(detector).__name__\n",
    "            matcher_type = type(matcher).__name__\n",
    "            print(f\"\\n\\nDetector: {detector_type}\")\n",
    "            print(f\"Matcher: {matcher_type}\")\n",
    "\n",
    "        ### Feature Extraction and Matching ###\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\n\\n### Feature Extraction and Matching ###\\n\")\n",
    "\n",
    "        # Run feature detection WITH undistortion\n",
    "        img1, kp1, des1, img2, kp2, des2 = feature_detection(\n",
    "            img1_path=f\"assets/{camera_path}/moai_view1.jpg\",\n",
    "            img2_path=f\"assets/{camera_path}/moai_view2.jpg\",\n",
    "            K=K,\n",
    "            dist=dist,\n",
    "            detector=detector,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        n_keypoints = len(kp1)\n",
    "\n",
    "        matches = feature_matcher(\n",
    "            img1, kp1, des1, img2, kp2, des2, matcher=matcher, debug=debug\n",
    "        )\n",
    "\n",
    "        n_matches = len(matches)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Time elapsed: {elapsed_time:.4f} s\\n\")\n",
    "            print(\n",
    "                f\"Number of keypoints: {len(kp1)}, Number of matches: {len(matches)}\\n\"\n",
    "            )\n",
    "\n",
    "        if debug:\n",
    "            visualize_matcher(img1, kp1, img2, kp2, matches)\n",
    "\n",
    "        ### Epipolar Geometry Estimation ###\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\n\\n### Epipolar Geometry Estimation ###\\n\")\n",
    "\n",
    "        # Extract matched points\n",
    "        pts1, pts2 = get_keypoint_coords_from_matches(kp1, kp2, matches)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Initial Matches: {len(pts1)}\")\n",
    "\n",
    "        F, pts1_in, pts2_in, mask = estimate_fundamental_matrix(\n",
    "            pts1, pts2, ransac_thresh=1.0, confidence=0.999\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"F inliers: {len(pts1_in)} / {len(pts1)}\")\n",
    "            print(f\"Fundamental Matrix:\\n{F}\\n\")\n",
    "\n",
    "        # Essential matrix directly\n",
    "        # We're using the inliers found with F, new inliers will be more polished\n",
    "        E, pts1_in, pts2_in, mask = estimate_essential_matrix(pts1_in, pts2_in, K)\n",
    "\n",
    "        n_inliers = len(pts1_in)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"E inliers: {len(pts1_in)} / {len(pts1)}\")\n",
    "            print(f\"Essential Matrix:\\n{E}\\n\")\n",
    "\n",
    "        # Recover pose\n",
    "        # This extracts the rotation and translation.\n",
    "        # It uses the points (pts1, pts2) to check which of the 4 solutions is valid.\n",
    "        # pts1 and pts2 should be the INLIERS from your previous steps.\n",
    "        num_points, R, t, mask = cv.recoverPose(E, pts1_in, pts2_in, K)\n",
    "\n",
    "        # Filter points again using the pose mask\n",
    "        mask_pose = mask.ravel().astype(bool)\n",
    "        pts1_valid = pts1_in[mask_pose]\n",
    "        pts2_valid = pts2_in[mask_pose]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Recovered Pose with {num_points} valid points.\")\n",
    "            print(f\"Rotation matrix R:\\n{R}\\n\")\n",
    "            print(f\"Translation vector t:\\n{t}\\n\")\n",
    "            print(f\"RecoverPose kept {num_points} points (Cheirality check)\")\n",
    "\n",
    "        if debug:\n",
    "            draw_epipolar_lines(img1, pts1_valid, img2, pts2_valid, F)\n",
    "\n",
    "        ### Triangulation and 3D Reconstruction ###\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\n\\n### Triangulation and 3D Reconstruction ###\\n\")\n",
    "\n",
    "        P1, P2 = estimate_projection_matrices(K, R, t)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Projection Matrix 1:\\n{P1}\\n\")\n",
    "            print(f\"Projection Matrix 2:\\n{P2}\\n\")\n",
    "\n",
    "        points_3d = triangulate_3d_points(P1, P2, pts1_valid, pts2_valid)\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                \"Valid points Z range:\",\n",
    "                np.min(points_3d[:, 2]),\n",
    "                np.max(points_3d[:, 2]),\n",
    "            )\n",
    "\n",
    "        # Positive depth filter\n",
    "        mask_z = points_3d[:, 2] > 0\n",
    "\n",
    "        points_3d = points_3d[mask_z]\n",
    "        pts1_used = pts1_valid[mask_z]\n",
    "        pts2_used = pts2_valid[mask_z]\n",
    "\n",
    "        # Filtering out extreme depth outliers using percentile\n",
    "        z = points_3d[:, 2]\n",
    "        upper = np.percentile(z, 97)\n",
    "\n",
    "        mask_depth = z < upper\n",
    "\n",
    "        points_3d = points_3d[mask_depth]\n",
    "        pts1_used = pts1_used[mask_depth]\n",
    "        pts2_used = pts2_used[mask_depth]\n",
    "\n",
    "        n_3d_points = points_3d.shape[0]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Generated {len(points_3d)} 3D points.\")\n",
    "\n",
    "        # Calculate quantitative error\n",
    "        mean_error_1, mean_error_2, total_mean_error = compute_reprojection_error(\n",
    "            P1, P2, points_3d, pts1_used, pts2_used\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Re-projection Error Camera 1: {mean_error_1:.4f} px\")\n",
    "            print(f\"Re-projection Error Camera 2: {mean_error_2:.4f} px\")\n",
    "            print(f\"Total Mean Re-projection Error: {total_mean_error:.4f} px\")\n",
    "\n",
    "        if debug:\n",
    "            plot_3d_point_cloud(points_3d)\n",
    "\n",
    "        # Append results\n",
    "        results.append(\n",
    "            [\n",
    "                detector.__class__.__name__,\n",
    "                matcher.__class__.__name__,\n",
    "                n_keypoints,\n",
    "                n_matches,\n",
    "                n_inliers,\n",
    "                n_3d_points,\n",
    "                f\"{total_mean_error:.2f} px\",\n",
    "                f\"{elapsed_time:.2f} s\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            results,\n",
    "            headers=[\n",
    "                \"Detector\",\n",
    "                \"Matcher\",\n",
    "                \"#Keypoints\",\n",
    "                \"#Matches\",\n",
    "                \"#Inliers\",\n",
    "                \"#3D Points\",\n",
    "                \"Reproj Error\",\n",
    "                \"Time\",\n",
    "            ],\n",
    "            tablefmt=\"github\",\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
